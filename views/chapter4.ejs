<%- include("header") -%>


            <!-- Sidebar -->
            <nav
            class="w3-sidebar activecolor w3-bar-block w3-collapse  w3-animate-left w3-light-grey"
            id="mySidebar"
          >
            <a
              href="javascript:void(0)"
              onclick="w3_close()"
              class="w3-right w3-xlarge w3-padding-large w3-hover-teal w3-hide-large"
              title="Close Menu"
            >
              <i class="fa fa-remove"></i>
            </a>
            <h4 class="w3-text-black">Algorithms</span> Introduction</h4>
            
            <a  target="_top" class="w3-bar-item w3-button w3-round-xxlarge" href="/chapter1#properties" >Properties</a>
            <a target="_top" class="w3-bar-item w3-button w3-round-xxlarge " href="/chapter1#development">Development</a>
            <a target="_top" class="w3-bar-item w3-button w3-round-xxlarge " href="/chapter1#analysis">Algorithm Analysis</a>
            
                
            <h4 class="w3-text-black">Data</span> Structures</h4>
            <a target="_top" class="w3-bar-item w3-button w3-round-xxlarge " href="/chapter2#Definition">Definition</a>
            <a target="_top" class="w3-bar-item w3-button w3-round-xxlarge " href="/chapter2#abstract">Abstract Data types</a>
            <a target="_top" class="w3-bar-item w3-button w3-round-xxlarge " href="/chapter2#linear">Linear Data Structures</a>
            <a target="_top" class="w3-bar-item w3-button w3-round-xxlarge " href="/chapter2#nonlinear">Non-Linear Data Structures</a>
            <a target="_top" class="w3-bar-item w3-button w3-round-xxlarge " href="/chapter2#operations">Operations on data structures</a>
          
            <h4 class="w3-text-black">All</span> Together</h4>
            <a target="_top" class="w3-bar-item w3-button w3-round-xxlarge" href="/chapter3#writing">Writing Algorithms</a>
            <a target="_top" class="w3-bar-item w3-button w3-round-xxlarge " href="/chapter3#example">Algorithms Example</a>
            <a target="_top" class="w3-bar-item w3-button w3-round-xxlarge " href="/chapter3#implementation">Algorithm Implementation</a>
            <a target="_top" class="w3-bar-item w3-button w3-round-xxlarge " href="/chapter3#strategy">Algorithmic Strategies</a>
        
            <h4 class="w3-text-black">Performance </span> Analysis</h4>
      
            <a target="_top" class="w3-bar-item w3-button w3-round-xxlarge active" href="/Chapter4#analysis ">Analysis of algorithms</a>
            <a target="_top" class="w3-bar-item w3-button w3-round-xxlarge" href="/Chapter4#empirical">Empirical/Experimental Analysis</a>
            <a target="_top" class="w3-bar-item w3-button w3-round-xxlarge" href="/Chapter4#analytical">Analytical Method</a>
            <a target="_top" class="w3-bar-item w3-button w3-round-xxlarge " href="/Chapter4#common">Common Functions used in analysis.</a>
            
            <h4 class="w3-text-black">Asymptotic </span>Notations</h4>
            <a target="_top" class="w3-bar-item w3-button w3-round-xxlarge " href="/chapter4#oh">Big oh Notation</a>
            <a target="_top" class="w3-bar-item w3-button w3-round-xxlarge " href="/chapter4#omega">Omega Notation</a>
            <a target="_top" class="w3-bar-item w3-button w3-round-xxlarge " href="/chapter4#theta">θ Notation</a>
      
          </nav>


    <!-- Overlay effect when opening sidebar on small screens -->
    <div class="w3-overlay w3-hide-large" onclick="w3_close()" style="cursor: pointer" title="close side menu"
      id="myOverlay"></div>

    <!-- Main content: shift it to the right by 250 pixels when the sidebar is visible -->
    <div class="w3-main" style="margin-left: 250px">
      <div class="w3-row w3-padding-64">
        <div class=" w3-container">
          <h2 class="w3-text-teal" id = "analysis">Analysis of algorithms</h2>
          <p>
            The efficiency of an algorithm can be decided by measuring the performance of an algorithm. We can measure
            the performance of an algorithm by computing two factors:
          </p>


          <ol type="1">
            <li>Amount of time required by an algorithm to execute.</li>
            <li>Amount of storage required by an algorithm.</li>
          </ol>
          <p>This is popularly known as <strong class="w3-text-teal"> time complexity and space complexity of
              an algoithm.</strong>
          </p>
          <img class="w3-md4" src="images/Image_006.jpg" alt="analysis of algorithms">
          <p class="w3-text-teal">There are many criteria upon which we can judge program. For instance</p>
          <p>
            Does my program work on what I want to do?<br>
            Does it work correctly according to original specifications?
            Are procedures created in such a way that they perform logical functioning.<br>
            Is the code readable?<br><br><br>

          </p>
          <p class="w3-text-teal">If one asks the question that how much time it takes to execute this statement. It
            is impossible to determine exact timing taken by the statement to execute unless we have the following
            information.</p>
          <p>
            The machine that is used to execute this statement,<br>
            Machine language instruction set. <br>
            The time required by each machine instruction.<br>
            The translation a compiler will make for this statement to machine language<br>
            And the kind of operating system (multi-programming or time sharing)
            The information may vary from machine to machine and perhaps we won’t get the exact figures.
            So the better ideas is to determine the time taken by each instruction to execute called the <strong
              class="w3-text-teal">‘frequency count’</strong> .
          </p>
          <p>There are basically two approaches of analyzing algorithms <br>
            <li>Empirical/ Experimental analysis</li>
            <li>Analytical approach</li>
          </p>

          <h3 id="empirical" class="w3-text-teal">Empirical/Experimental Analysis</h3>
          <p>
            One way to study the efficiency of an algorithm is to implement it and experiment by running the program on
            various test inputs while recording the time spent during each execution <br><br>
            Because we are interested in the general dependence of running time on the size and structure of the input,
            we should perform independent experiments on many different test inputs of various sizes.<br><br>
            The key is that if we record the time immediately before executing the algorithm and then immediately after,
            we can measure the elapsed time of an algorithm’s execution by computing the difference of those times.
            <br>
            <br>
            <li>Running time is measured for different sets of input values.</li>
            <li>Actual time is plotted against the input size to analyze growth rate</li>
            <img class="w3-md4" src="images/Image_009.jpg" alt="running time analysis">
          <pre>
                However, the measured times reported by any two methods will vary greatly from machine to machine, and may likely vary from trial to trial, even on the same machine. (CPU – Memory).

                Experiments are quite useful when comparing the efficiency of two or more algorithms, so long as they are gathered under similar circumstances.

                Experimental running times of two algorithms are difficult to directly compare unless the experiments are performed in the same hardware and software environments.
   
                An algorithm must be fully implemented in order to execute it to study
                its running time experimentally.

                Experiments can be done only on a limited set of test inputs; hence, they leave out the running times of inputs not included in the experiment (and these inputs may be important).


              </pre>
          <p class="w3-text-teal"> Our goal is to develop an approach to analysing the efficiency of algorithms that:
          </p>
          <pre>
                Allows us to evaluate the relative efficiency of any two algorithms in a way that is independent of the hardware and software environment.
                Takes into account all possible inputs.

                Characterizes running time as a function of the input size, n.

                This calls for an analytical method that does not require
                implementation of an algorithm
                
                <h3 id="analytical" class="w3-text-teal">Analytical Method</h3>
                
                This analysis uses mathematical techniques to estimate the running time.
                It	identifies	time	consuming	crucial operations in an algorithm.
              	The	number	of	key	operations	can	be determined by analyzing the pseudo code.
                Is performed by studying a high-level description of the algorithm without need for implementation.

              </pre>

          To analyse the running time of an algorithm without performing experiments, we perform an analysis directly on
          a high-level description of the algorithm (either in the form of an actual code fragment, or
          language-independent pseudocode).

          <strong>Primitive operation</strong> is a low-level instruction with constant time.

          Counting the primitive operations can be used as measure for algorithm performance.

          <h4 class="w3-text-teal"> We define a set of primitive operations such as the following:</h4>

          <strong>
            <li>Assigning a value to a variable</li>
            <li>Following an object reference</li>
            <li>Performing an arithmetic operation (for example, adding two numbers)</li>
            <li>Comparing two numbers</li>
            <li>Accessing a single element of an array by index</li>
            <li>Calling a method</li>
            <li>Returning from a method</li>
          </strong>

          Frequency count in analytical method is highly recommended as it gives the efficiency of an algorithm without
          necessarily being machine dependent.
          By inspecting the pseudocode, we can determine the maximum number of primitive operations executed by an
          algorithm, as a function of the input size.
          <br>
          <br>
          
          
        </p>
      </div>
      
      </div>
      <!-- end section -->
      
      <!-- New section -->
      <div class="w3-row">
        <div class="w3-container">
          <h3 class="w3-text-teal" id="#common">Common Functions used in analysis.</h3>
          
          <ol></ol>
        <h3 class="w3-text-teal">The Constant Function f(n) = C</h3>
          For any argument n, the constant function f(n) assigns the value C. It doesn't matter what the input size n is, f(n) will always be equal to the constant value C. <br>
          The most fundamental constant function is f(n) = 1, and this is the
          typical constant function that is used<br>
          The constant function is useful in algorithm analysis, because it characterizes the number of steps needed to do a basic operation on a computer, like adding two numbers, assigning a value to some variable, or comparing two numbers. Executing one instruction a fixed number of times also needs constant time only.<br>
          Constant algorithm does not depend on the input size.<br>
          Examples: arithmetic calculation, comparison, variable
          declaration, assignment statement, invoking a method or function.
          
          
        <h3 class="w3-text-teal">The Logarithm Function f(n) = logn</h3>
          Running time varies as log N, where N is size of problem.<br>
          It grows slowly, and has the best performance.<br>
          If	the	problem	size	increases	by	factor	of	1000,	running	time increases by 10<br>
          Example: Binary search algorithm<br>
        <h3 class="w3-text-teal">Linear Algorithm O(N)</h3>
          Execution time grows in direct proportion to the size of a problem.<br>
          Performance is rated as good<br>
          Algorithm, which are based on a single loop, shows linear growth rate.<br>
          Examples are: searching, deleting and inserting operations in an
          array
          
          
        <h3 class="w3-text-teal">The N-Log-N Function f(n) = nlogn</h3>
            This function grows a little faster than the linear function and a lot
            slower than the quadratic function (n2)<br>
            Example: Divide – and – Conquer algorithms show this behavior like the merge sort, quick sort etc.
          <h3 class="w3-text-teal">The Quadratic Function f(n) = n2</h3>
            It appears a lot in the algorithm analysis, since there are many algorithms that have nested loops, where the inner loop performs a linear number of operations and the outer loop is performed a linear  number  of  times.  In  such  cases,   the   algorithm performs n*n = n2 operations.<br>
            Quadratic algorithms are practical for relatively small problems. Whenever n doubles, the running time increases fourfold.<br>
            Example: some manipulations of the n by n array.


          <h3 class="w3-text-teal">The Cubic Function and Other Polynomials</h3>
            The cubic function f(n) = n3. This function appears less frequently in the context of the algorithm analysis than the constant, linear, and quadratic functions. It's practical for use only on small problems.<br>
            Whenever n doubles, the running time increases eightfold.<br>
            Example: n by n matrix multiplication.
          <h3 class="w3-text-teal">The Exponential Function f(n) = bn</h3>
            In this function, b is a positive constant, called the base, and the argument n is the exponent.<br>
            Exponential algorithm is usually not appropriate for practical use.<br>
            Example: Towers of the Hanoi.


          <h3 class="w3-text-teal">The Factorial Function f(n) = n!</h3>
            Factorial  function  is  even  worse  than  the  exponential  function. Whenever n increases by 1, the running time increases by a factor of n.<br>
            For example, permutations of n elements.



          <p >
            To capture the order of growth of an algorithm’s running time, we will associate,
             with each algorithm, a function f (n) that characterizes the number of primitive operations that are performed as a function of the input size n. <br>
            Often we need to know how running time varies with the variation of
            input size.<br>
            Measuring the performance of an algorithm in relation with the input size n is called order of growth <br>
            Behavior of an algorithm for large input is called asymptotic growth <br>

            If running time of a function grows as the size of input size N increases, we say that the algorithm asymptotically varies as N

          </p>

          <p>
            <p>Usually the time required by an algorithm falls under these 3 types namely;
              <ul>
                <li>Best case:	Minimum time required for program execution.</li><li>Average case:	Average time required for program execution.</li><li>Worst case:	Maximum time required for program execution.</li>
              </ul>
            </p><br>
            <p>And the following asymptotic notations are commmonly used to represent the above cases for a given algorithm;
              <ul>
                <li class="w3-text-teal">Big Oh notation, O</li><li class="w3-text-teal">Omega notation, Ω</li><li class="w3-text-teal">Theta notation, θ</li>
              </ul>
            </p><br><br>
            <h3 id="oh" class="w3-text-teal">Big Oh notation, O</h3>
            <p>he notation Ο(n) is the formal way to express the upper bound of an algorithm's running time. It measures the worst case time complexity or the longest amount of time an algorithm can possibly take to complete.<br>For example, for a function f(n)</p>
           i.e Ο(f(n)) = { g(n) : there exists c > 0 and n0 such that f(n) ≤ c.g(n) for all n > n0. }</p><br>
            <h3 id = "omega"class="w3-text-teal">Omega notation, Ω</h3>
            <p>The notation Ω(n) is the formal way to express the lower bound of an algorithm's running time. It measures the best case time complexity or the best amount of time an algorithm can possibly take to complete.</p>
            i.e Ω(f(n)) ≥ { g(n) : there exists c > 0 and n0 such that g(n) ≤ c.f(n) for all n > n0. }</p><br>
            <h3 id ="theta" class="w3-text-teal">Theta notation, θ</h3>
            <p>The notation θ(n) is the formal way to express both the lower bound and the upper bound of an algorithm's running time. It is represented as follows</p>
            i.e θ(f(n)) = { g(n) if and only if g(n) =  Ο(f(n)) and g(n) = Ω(f(n)) for all n > n0. }</p><br>
            <br><br>
          </p>
            
          

          <div>
          </div>

        </div>

        <!-- End section -->

      </div>


      <div class="w3-clear nextprev w3-padding-32">
        <a class="w3-left w3-button w3-medium w3-round-xxlarge" style="background-color: #009494;" href="/chapter3">
          Previous</a>
        <a class="w3-right w3-button w3-medium w3-round-xxlarge" style="background-color: #009494;"
          href="/chapter5">Next</a>
      </div>

      <%- include("footer") -%>